---

# Tutorial: ONNX Runtime Execution Providers

## English

This guide provides a concise overview of how to use ONNX Runtime Execution Providers to enhance model inference performance across various hardware platforms.

### Supported Execution Providers

    - CUDA:                      Leverage NVIDIA GPUs for accelerated inference.
    - TensorRT:                  Optimize inference on NVIDIA GPUs using TensorRT.
    - Android-Qualcomm-QNN:      Utilize Qualcomm's Neural Processing SDK on Android devices.

### Upcoming Support

The following execution providers are planned for future integration:

    - Intel-OpenVINO:            For optimized inference on Intel hardware.
    - Windows-DirectML:          Leverage DirectML on Windows platforms.
    - Apple-CoreML:              Integrate CoreML for Apple devices.
    - AMD-ROCm:                  Use AMD's ROCm platform for GPU acceleration.
    - AMD-MIGraphX:              Accelerate inference on AMD GPUs with MIGraphX.
    - AMD-VitisAI:               Integrate Vitis AI for Xilinx devices.
    - Android-XNNPACK:           Optimize inference on Android with XNNPACK.

---

## Chinese

配置ONNX Runtime执行提供程序，在各种硬件平台上优化模型推理性能。

### 支持的执行提供程序

    - CUDA:                      利用NVIDIA GPU进行加速推理。
    - TensorRT:                  使用TensorRT优化NVIDIA GPU上的推理。
    - Android-Qualcomm-QNN:      在Android设备上使用Qualcomm的神经处理SDK。

### 即将支持

以下执行提供程序计划在未来集成：

    - Intel-OpenVINO:            针对Intel硬件进行优化推理。
    - Windows-DirectML:          在Windows平台上利用DirectML。
    - Apple-CoreML:              为Apple设备集成CoreML。
    - AMD-ROCm:                  使用AMD的ROCm平台进行GPU加速。
    - AMD-MIGraphX:              使用MIGraphX加速AMD GPU上的推理。
    - AMD-VitisAI:               为AMD设备集成Vitis AI。
    - Android-XNNPACK:           使用XNNPACK优化Android上的推理。

---

## Japanese

さまざまなハードウェアプラットフォームでモデル推論のパフォーマンスを最適化するために、ONNX Runtime Execution Providersを使用するための簡潔なガイド。

### サポートされている実行プロバイダ

    - CUDA:                      NVIDIA GPUを活用して推論を加速します。
    - TensorRT:                  TensorRTを使用してNVIDIA GPUでの推論を最適化します。
    - Android-Qualcomm-QNN:      AndroidデバイスでQualcommのニューラルプロセッシングSDKを利用します。

### 今後のサポート

次の実行プロバイダは将来的に統合される予定です：

    - Intel-OpenVINO:            Intelハードウェアでの推論を最適化します。
    - Windows-DirectML:          WindowsプラットフォームでDirectMLを活用します。
    - Apple-CoreML:              Appleデバイス用にCoreMLを統合します。
    - AMD-ROCm:                  AMDのROCmプラットフォームを使用してGPUを加速します。
    - AMD-MIGraphX:              MIGraphXを使用してAMD GPUでの推論を加速します。
    - AMD-VitisAI:               AMDデバイス用にVitis AIを統合します。
    - Android-XNNPACK:           AndroidでXNNPACKを使用して推論を最適化します。

---

## Supported Execution Providers

    - CUDA:                      Leverage NVIDIA GPUs for accelerated inference.
    - TensorRT:                  Optimize inference on NVIDIA GPUs using TensorRT.
    - Android-Qualcomm-QNN:      Utilize Qualcomm's Neural Processing SDK on Android devices.

---

## Upcoming Support

The following execution providers are planned for future integration:

    - Intel-OpenVINO:            For optimized inference on Intel hardware.
    - Windows-DirectML:          Leverage DirectML on Windows platforms.
    - Apple-CoreML:              Integrate CoreML for Apple devices.
    - AMD-ROCm:                  Use AMD's ROCm platform for GPU acceleration.
    - AMD-MIGraphX:              Accelerate inference on AMD GPUs with MIGraphX.
    - AMD-VitisAI:               Integrate Vitis AI for AMD devices.
    - Android-XNNPACK:           Optimize inference on Android with XNNPACK.

---
